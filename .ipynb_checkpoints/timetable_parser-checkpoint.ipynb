{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648e5f08",
   "metadata": {},
   "source": [
    "# TransXChange XML â†’ CSV with PySpark\n",
    "\n",
    "This notebook parses all XML files in the Falcon Buses folder and produces clean CSV tables for:\n",
    "- `stops`\n",
    "- `lines`\n",
    "- `routes`\n",
    "- `route_links`\n",
    "- `journey_patterns`\n",
    "- `timing_links`\n",
    "- `operators`\n",
    "- `services`\n",
    "- `service_lines`\n",
    "- `service_journey_patterns`\n",
    "- `operating_profile`\n",
    "- `serviced_organisations`\n",
    "- `serviced_org_working_days`\n",
    "- `vehicle_journeys`\n",
    "\n",
    "The outputs are saved under `parsed_data/<table_name>/` as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d380618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 15:19:05 WARN Utils: Your hostname, Biplovs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.76 instead (on interface en0)\n",
      "26/02/06 15:19:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/06 15:19:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/06 15:19:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Setup Spark session (similar to start_check.ipynb)\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder.appName(\"transxchange_parse\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c28a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 XML files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_london/Falcon Buses_237/21332_106265_2026-01-02_16-02-13_current/FALC_12_FALCPK11099692512_20250831_-_2139939.xml',\n",
       " '/Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_london/Falcon Buses_237/21332_106265_2026-01-02_16-02-13_current/FALC_12_FALCPK11099692512_20250831_-_2139940.xml',\n",
       " '/Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_london/Falcon Buses_237/21332_106265_2026-01-02_16-02-13_current/FALC_12_FALCPK11099692512_20251229_20251231_2251476.xml',\n",
       " '/Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_london/Falcon Buses_237/21332_106265_2026-01-02_16-02-13_current/FALC_28_FALCPK1109969828_20250804_-_2142541.xml',\n",
       " '/Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_london/Falcon Buses_237/21332_106265_2026-01-02_16-02-13_current/FALC_28_FALCPK1109969828_20250804_-_2142647.xml']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration\n",
    "# ROOT_FOLDER = \"/Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/timetable_london/Falcon Buses_237/21332_106265_2026-01-02_16-02-13_current\"\n",
    "ROOT_FOLDER = os.path.join(os.getcwd(), \"../timetable_london/Falcon Buses_237/21332_106265_2026-01-02_16-02-13_current\")\n",
    "OUTPUT_BASE = os.path.join(os.getcwd(), \"../timetable_parsed_data\")\n",
    "\n",
    "\n",
    "xml_files = sorted(glob.glob(os.path.join(ROOT_FOLDER, \"*.xml\")))\n",
    "print(f\"Found {len(xml_files)} XML files\")\n",
    "xml_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7e5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ns(root):\n",
    "    match = re.match(r\"\\{.*\\}\", root.tag)\n",
    "    return match.group(0) if match else \"\"\n",
    "\n",
    "def _text(elem):\n",
    "    if elem is None or elem.text is None:\n",
    "        return None\n",
    "    return elem.text.strip()\n",
    "\n",
    "def _findtext(elem, path, ns):\n",
    "    return _text(elem.find(path, ns))\n",
    "\n",
    "def _strip_ns(tag):\n",
    "    return tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n",
    "\n",
    "def _children_tag_names(elem):\n",
    "    if elem is None:\n",
    "        return None\n",
    "    names = [_strip_ns(child.tag) for child in list(elem)]\n",
    "    return \",\".join(names) if names else None\n",
    "\n",
    "def _date_ranges(elem, ns):\n",
    "    if elem is None:\n",
    "        return None\n",
    "    ranges = []\n",
    "    for date_range in elem.findall(\"txc:DateRange\", ns):\n",
    "        start = _findtext(date_range, \"txc:StartDate\", ns)\n",
    "        end = _findtext(date_range, \"txc:EndDate\", ns)\n",
    "        if start or end:\n",
    "            ranges.append(f\"{start}:{end}\")\n",
    "    return \",\".join(ranges) if ranges else None\n",
    "\n",
    "def parse_transxchange_file(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ns_url = _get_ns(root).strip(\"{}\")\n",
    "    ns = {\"txc\": ns_url} if ns_url else {}\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    tables = {\n",
    "        \"stops\": [],\n",
    "        \"lines\": [],\n",
    "        \"routes\": [],\n",
    "        \"route_links\": [],\n",
    "        \"journey_patterns\": [],\n",
    "        \"timing_links\": [],\n",
    "        \"operators\": [],\n",
    "        \"services\": [],\n",
    "        \"service_lines\": [],\n",
    "        \"service_journey_patterns\": [],\n",
    "        \"operating_profile\": [],\n",
    "        \"serviced_organisations\": [],\n",
    "        \"serviced_org_working_days\": [],\n",
    "        \"vehicle_journeys\": [],\n",
    "    }\n",
    "\n",
    "    # Stops\n",
    "    for stop in root.findall(\".//txc:AnnotatedStopPointRef\", ns):\n",
    "        tables[\"stops\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"stop_id\": _findtext(stop, \"txc:StopPointRef\", ns),\n",
    "            \"common_name\": _findtext(stop, \"txc:CommonName\", ns),\n",
    "            \"latitude\": _findtext(stop, \".//txc:Latitude\", ns),\n",
    "            \"longitude\": _findtext(stop, \".//txc:Longitude\", ns),\n",
    "        })\n",
    "\n",
    "    # Lines\n",
    "    for line in root.findall(\".//txc:Line\", ns):\n",
    "        tables[\"lines\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"line_id\": line.get(\"id\"),\n",
    "            \"line_name\": _findtext(line, \"txc:LineName\", ns),\n",
    "            \"line_ref\": _findtext(line, \"txc:LineRef\", ns),\n",
    "            \"inbound_description\": _findtext(line, \"txc:InboundDescription\", ns),\n",
    "            \"outbound_description\": _findtext(line, \"txc:OutboundDescription\", ns),\n",
    "        })\n",
    "\n",
    "    # Routes\n",
    "    for route in root.findall(\".//txc:Route\", ns):\n",
    "        tables[\"routes\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"route_id\": route.get(\"id\"),\n",
    "            \"route_section_ref\": _findtext(route, \"txc:RouteSectionRef\", ns),\n",
    "            \"description\": _findtext(route, \"txc:Description\", ns),\n",
    "        })\n",
    "\n",
    "    # Route links (RouteSections)\n",
    "    for section in root.findall(\".//txc:RouteSection\", ns):\n",
    "        section_id = section.get(\"id\")\n",
    "        for link in section.findall(\"txc:RouteLink\", ns):\n",
    "            tables[\"route_links\"].append({\n",
    "                \"file_name\": file_name,\n",
    "                \"route_section_id\": section_id,\n",
    "                \"route_link_id\": link.get(\"id\"),\n",
    "                \"from_stop\": _findtext(link, \"txc:From/txc:StopPointRef\", ns),\n",
    "                \"to_stop\": _findtext(link, \"txc:To/txc:StopPointRef\", ns),\n",
    "                \"distance\": _findtext(link, \"txc:Distance\", ns),\n",
    "            })\n",
    "\n",
    "    # Journey patterns (Service > StandardService > JourneyPattern)\n",
    "    for jp in root.findall(\".//txc:JourneyPattern\", ns):\n",
    "        tables[\"journey_patterns\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"journey_pattern_id\": jp.get(\"id\"),\n",
    "            \"destination_display\": _findtext(jp, \"txc:DestinationDisplay\", ns),\n",
    "            \"direction\": _findtext(jp, \"txc:Direction\", ns),\n",
    "            \"route_ref\": _findtext(jp, \"txc:RouteRef\", ns),\n",
    "            \"section_refs\": _findtext(jp, \"txc:JourneyPatternSectionRefs\", ns),\n",
    "            \"operator_ref\": _findtext(jp, \"txc:OperatorRef\", ns),\n",
    "        })\n",
    "\n",
    "    # Timing links (JourneyPatternTimingLink)\n",
    "    for jp_section in root.findall(\".//txc:JourneyPatternSection\", ns):\n",
    "        jp_section_id = jp_section.get(\"id\")\n",
    "        for timing_link in jp_section.findall(\"txc:JourneyPatternTimingLink\", ns):\n",
    "            from_elem = timing_link.find(\"txc:From\", ns)\n",
    "            to_elem = timing_link.find(\"txc:To\", ns)\n",
    "            tables[\"timing_links\"].append({\n",
    "                \"file_name\": file_name,\n",
    "                \"journey_pattern_section_id\": jp_section_id,\n",
    "                \"timing_link_id\": timing_link.get(\"id\"),\n",
    "                \"route_link_ref\": _findtext(timing_link, \"txc:RouteLinkRef\", ns),\n",
    "                \"from_stop\": _findtext(timing_link, \"txc:From/txc:StopPointRef\", ns),\n",
    "                \"to_stop\": _findtext(timing_link, \"txc:To/txc:StopPointRef\", ns),\n",
    "                \"from_sequence\": from_elem.get(\"SequenceNumber\") if from_elem is not None else None,\n",
    "                \"to_sequence\": to_elem.get(\"SequenceNumber\") if to_elem is not None else None,\n",
    "                \"from_activity\": _findtext(from_elem, \"txc:Activity\", ns) if from_elem is not None else None,\n",
    "                \"to_activity\": _findtext(to_elem, \"txc:Activity\", ns) if to_elem is not None else None,\n",
    "                \"from_timing_status\": _findtext(from_elem, \"txc:TimingStatus\", ns) if from_elem is not None else None,\n",
    "                \"to_timing_status\": _findtext(to_elem, \"txc:TimingStatus\", ns) if to_elem is not None else None,\n",
    "                \"from_fare_stage\": _findtext(from_elem, \"txc:FareStageNumber\", ns) if from_elem is not None else None,\n",
    "                \"to_fare_stage\": _findtext(to_elem, \"txc:FareStageNumber\", ns) if to_elem is not None else None,\n",
    "                \"run_time\": _findtext(timing_link, \"txc:RunTime\", ns),\n",
    "            })\n",
    "\n",
    "    # Operators\n",
    "    for operator in root.findall(\".//txc:Operator\", ns):\n",
    "        tables[\"operators\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"operator_id\": operator.get(\"id\"),\n",
    "            \"operator_code\": _findtext(operator, \"txc:OperatorCode\", ns),\n",
    "            \"operator_short_name\": _findtext(operator, \"txc:OperatorShortName\", ns),\n",
    "            \"operator_name\": _findtext(operator, \"txc:OperatorName\", ns),\n",
    "            \"licence_number\": _findtext(operator, \"txc:LicenceNumber\", ns),\n",
    "            \"national_operator_code\": _findtext(operator, \"txc:NationalOperatorCode\", ns),\n",
    "        })\n",
    "\n",
    "    # Services + Operating Profile\n",
    "    for service in root.findall(\".//txc:Service\", ns):\n",
    "        service_code = _findtext(service, \"txc:ServiceCode\", ns)\n",
    "        tables[\"services\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"service_id\": service.get(\"id\"),\n",
    "            \"service_code\": service_code,\n",
    "            \"service_description\": _findtext(service, \"txc:Description\", ns),\n",
    "            \"line_ref\": _findtext(service, \".//txc:LineRef\", ns),\n",
    "            \"start_date\": _findtext(service, \".//txc:OperatingPeriod/txc:StartDate\", ns),\n",
    "            \"end_date\": _findtext(service, \".//txc:OperatingPeriod/txc:EndDate\", ns),\n",
    "            \"registered_operator_ref\": _findtext(service, \"txc:RegisteredOperatorRef\", ns),\n",
    "            \"public_use\": _findtext(service, \"txc:PublicUse\", ns),\n",
    "        })\n",
    "\n",
    "        for service_line in service.findall(\"txc:Lines/txc:Line\", ns):\n",
    "            tables[\"service_lines\"].append({\n",
    "                \"file_name\": file_name,\n",
    "                \"service_code\": service_code,\n",
    "                \"line_name\": _findtext(service_line, \"txc:LineName\", ns),\n",
    "                \"inbound_description\": _findtext(service_line, \"txc:InboundDescription\", ns),\n",
    "                \"outbound_description\": _findtext(service_line, \"txc:OutboundDescription\", ns),\n",
    "            })\n",
    "\n",
    "        standard_service = service.find(\"txc:StandardService\", ns)\n",
    "        if standard_service is not None:\n",
    "            origin = _findtext(standard_service, \"txc:Origin\", ns)\n",
    "            destination = _findtext(standard_service, \"txc:Destination\", ns)\n",
    "            for idx, jp in enumerate(standard_service.findall(\"txc:JourneyPattern\", ns), start=1):\n",
    "                tables[\"service_journey_patterns\"].append({\n",
    "                    \"file_name\": file_name,\n",
    "                    \"service_code\": service_code,\n",
    "                    \"journey_pattern_index\": str(idx),\n",
    "                    \"origin\": origin,\n",
    "                    \"destination\": destination,\n",
    "                    \"destination_display\": _findtext(jp, \"txc:DestinationDisplay\", ns),\n",
    "                    \"direction\": _findtext(jp, \"txc:Direction\", ns),\n",
    "                    \"route_ref\": _findtext(jp, \"txc:RouteRef\", ns),\n",
    "                    \"section_refs\": _findtext(jp, \"txc:JourneyPatternSectionRefs\", ns),\n",
    "                    \"operator_ref\": _findtext(jp, \"txc:OperatorRef\", ns),\n",
    "                })\n",
    "\n",
    "        operating_profile = service.find(\"txc:OperatingProfile\", ns)\n",
    "        if operating_profile is not None:\n",
    "            regular_days = _children_tag_names(operating_profile.find(\"txc:RegularDayType/txc:DaysOfWeek\", ns))\n",
    "            serviced_org_ref = _findtext(\n",
    "                operating_profile,\n",
    "                \"txc:ServicedOrganisationDayType/txc:DaysOfNonOperation/txc:WorkingDays/txc:ServicedOrganisationRef\",\n",
    "                ns,\n",
    "            )\n",
    "            special_non_op = _date_ranges(\n",
    "                operating_profile.find(\"txc:SpecialDaysOperation/txc:DaysOfNonOperation\", ns),\n",
    "                ns,\n",
    "            )\n",
    "            bank_op = _children_tag_names(operating_profile.find(\"txc:BankHolidayOperation/txc:DaysOfOperation\", ns))\n",
    "            bank_non_op = _children_tag_names(operating_profile.find(\"txc:BankHolidayOperation/txc:DaysOfNonOperation\", ns))\n",
    "            tables[\"operating_profile\"].append({\n",
    "                \"file_name\": file_name,\n",
    "                \"service_code\": service_code,\n",
    "                \"regular_days\": regular_days,\n",
    "                \"serviced_org_ref\": serviced_org_ref,\n",
    "                \"special_days_non_operation\": special_non_op,\n",
    "                \"bank_holiday_operation\": bank_op,\n",
    "                \"bank_holiday_non_operation\": bank_non_op,\n",
    "            })\n",
    "\n",
    "    # Serviced organisations + working days\n",
    "    for org in root.findall(\".//txc:ServicedOrganisation\", ns):\n",
    "        org_code = _findtext(org, \"txc:OrganisationCode\", ns)\n",
    "        tables[\"serviced_organisations\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"organisation_code\": org_code,\n",
    "            \"name\": _findtext(org, \"txc:Name\", ns),\n",
    "        })\n",
    "\n",
    "        for date_range in org.findall(\"txc:WorkingDays/txc:DateRange\", ns):\n",
    "            tables[\"serviced_org_working_days\"].append({\n",
    "                \"file_name\": file_name,\n",
    "                \"organisation_code\": org_code,\n",
    "                \"start_date\": _findtext(date_range, \"txc:StartDate\", ns),\n",
    "                \"end_date\": _findtext(date_range, \"txc:EndDate\", ns),\n",
    "            })\n",
    "\n",
    "    # Vehicle journeys\n",
    "    for journey in root.findall(\".//txc:VehicleJourney\", ns):\n",
    "        journey_code = _findtext(journey, \"txc:VehicleJourneyCode\", ns)\n",
    "        tables[\"vehicle_journeys\"].append({\n",
    "            \"file_name\": file_name,\n",
    "            \"vehicle_journey_code\": journey_code,\n",
    "            \"service_ref\": _findtext(journey, \"txc:ServiceRef\", ns),\n",
    "            \"line_ref\": _findtext(journey, \"txc:LineRef\", ns),\n",
    "            \"journey_pattern_ref\": _findtext(journey, \"txc:JourneyPatternRef\", ns),\n",
    "            \"departure_time\": _findtext(journey, \"txc:DepartureTime\", ns),\n",
    "            \"operator_ref\": _findtext(journey, \"txc:OperatorRef\", ns),\n",
    "            \"journey_ticket_machine_code\": _findtext(journey, \"txc:Operational/txc:TicketMachine/txc:JourneyCode\", ns),\n",
    "        })\n",
    "\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb9b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 10 / 103 files\n",
      "Parsed 20 / 103 files\n",
      "Parsed 30 / 103 files\n",
      "Parsed 40 / 103 files\n",
      "Parsed 50 / 103 files\n",
      "Parsed 60 / 103 files\n",
      "Parsed 70 / 103 files\n",
      "Parsed 80 / 103 files\n",
      "Parsed 90 / 103 files\n",
      "Parsed 100 / 103 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stops': 10588,\n",
       " 'lines': 103,\n",
       " 'routes': 602,\n",
       " 'route_links': 28031,\n",
       " 'journey_patterns': 602,\n",
       " 'timing_links': 28031,\n",
       " 'operators': 103,\n",
       " 'services': 103,\n",
       " 'service_lines': 103,\n",
       " 'service_journey_patterns': 602,\n",
       " 'operating_profile': 103,\n",
       " 'serviced_organisations': 29,\n",
       " 'serviced_org_working_days': 174,\n",
       " 'vehicle_journeys': 1718}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse all files and collect rows for each table\n",
    "all_tables = {\n",
    "    \"stops\": [],\n",
    "    \"lines\": [],\n",
    "    \"routes\": [],\n",
    "    \"route_links\": [],\n",
    "    \"journey_patterns\": [],\n",
    "    \"timing_links\": [],\n",
    "    \"operators\": [],\n",
    "    \"services\": [],\n",
    "    \"service_lines\": [],\n",
    "    \"service_journey_patterns\": [],\n",
    "    \"operating_profile\": [],\n",
    "    \"serviced_organisations\": [],\n",
    "    \"serviced_org_working_days\": [],\n",
    "    \"vehicle_journeys\": [],\n",
    "}\n",
    "\n",
    "for i, file_path in enumerate(xml_files, start=1):\n",
    "    file_tables = parse_transxchange_file(file_path)\n",
    "    for table_name, rows in file_tables.items():\n",
    "        all_tables[table_name].extend(rows)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Parsed {i} / {len(xml_files)} files\")\n",
    "\n",
    "{name: len(rows) for name, rows in all_tables.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1883f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stops -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/stops\n",
      "Saved lines -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/lines\n",
      "Saved routes -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/routes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 15:19:13 WARN TaskSetManager: Stage 3 contains a task of very large size (1536 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved route_links -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/route_links\n",
      "Saved journey_patterns -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/journey_patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 15:19:15 WARN TaskSetManager: Stage 5 contains a task of very large size (3214 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved timing_links -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/timing_links\n",
      "Saved operators -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/operators\n",
      "Saved services -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/services\n",
      "Saved service_lines -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/service_lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 15:19:18 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved service_journey_patterns -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/service_journey_patterns\n",
      "Saved operating_profile -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/operating_profile\n",
      "Saved serviced_organisations -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/serviced_organisations\n",
      "Saved serviced_org_working_days -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/serviced_org_working_days\n",
      "Saved vehicle_journeys -> /Users/biplovgautam/Desktop/college/4 semester/BigDataCourseWork/parser_notebooks/../timetable_parsed_data/vehicle_journeys\n"
     ]
    }
   ],
   "source": [
    "# Write each table to CSV (one folder per table)\n",
    "def write_table(table_name, rows):\n",
    "    if not rows:\n",
    "        print(f\"Skipping {table_name}: no rows\")\n",
    "        return\n",
    "\n",
    "    # Build a stable schema (all columns as strings) to avoid inference errors\n",
    "    field_names = sorted({key for row in rows for key in row.keys()})\n",
    "    schema = StructType([StructField(name, StringType(), True) for name in field_names])\n",
    "    normalized_rows = [\n",
    "        {name: (None if row.get(name) is None else str(row.get(name))) for name in field_names}\n",
    "        for row in rows\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(normalized_rows, schema=schema)\n",
    "    output_path = os.path.join(OUTPUT_BASE, table_name)\n",
    "    (\n",
    "        df.coalesce(1)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .csv(output_path)\n",
    "    )\n",
    "    print(f\"Saved {table_name} -> {output_path}\")\n",
    "\n",
    "for table_name, rows in all_tables.items():\n",
    "    write_table(table_name, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b3e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: stop Spark when finished\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af6639-f0de-4937-81dc-0864051e0ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d0b98-987b-4cd5-9787-4e1b1716fbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
