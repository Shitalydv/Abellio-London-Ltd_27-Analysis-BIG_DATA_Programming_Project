{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648e5f08",
   "metadata": {},
   "source": [
    "# TransXChange XML Parser - Abellio London Ltd\n",
    "\n",
    "This notebook parses all TransXChange XML files from **Abellio London Ltd_27** folder and extracts data into CSV files.\n",
    "\n",
    "## What it does:\n",
    "1. üîç Scans all XML files in the Abellio London Ltd_27 folder\n",
    "2. üìä Extracts 14 different data tables from each XML file\n",
    "3. üîÑ Consolidates data from all XML files\n",
    "4. üíæ Exports each table type to separate CSV files\n",
    "\n",
    "## Tables extracted:\n",
    "- **stops** - Bus stop information\n",
    "- **operators** - Operator details (Abellio London Limited)\n",
    "- **services** - Service codes and routes\n",
    "- **lines** - Line numbers and descriptions\n",
    "- **routes** - Route definitions\n",
    "- **route_links** - Links between stops on routes\n",
    "- **journey_patterns** - Journey pattern definitions\n",
    "- **timing_links** - Timing information between stops\n",
    "- **vehicle_journeys** - Individual journey schedules\n",
    "- **serviced_organisations** - Organizations served\n",
    "- **serviced_org_working_days** - Working day definitions\n",
    "- **operating_profiles** - Operating schedules\n",
    "- **service_journey_patterns** - Service to journey pattern mappings\n",
    "- **service_lines** - Service to line mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d380618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully!\n",
      "‚úì Python version: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 11:09:21) [Clang 14.0.6 ]\n",
      "‚úì Pandas version: 2.2.3\n",
      "‚úì Working directory: /Users/shitalyadav/Desktop/untitled folder\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"‚úì Python version: {sys.version}\")\n",
    "print(f\"‚úì Pandas version: {pd.__version__}\")\n",
    "print(f\"‚úì Working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c28a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found 11 XML files in Abellio London Ltd_27 folder\n",
      "‚úì Output directory: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data\n",
      "\n",
      "Files to process:\n",
      "  1. 404-21-404-_-y05-2_1.xml\n",
      "  2. tfl_21-404-_-y05-6_qyPH39c.xml\n",
      "  3. tfl_21-404-_-y05-7.xml\n",
      "  4. tfl_21-407-_-y05-11_HygcEjI.xml\n",
      "  5. tfl_21-464-_-y05-55535.xml\n",
      "  6. tfl_21-464-_-y05-55538.xml\n",
      "  7. tfl_21-464-_-y05-55539.xml\n",
      "  8. tfl_21-465-_-y05-7_b3z8CFQ.xml\n",
      "  9. tfl_21-R68-_-y05-58885.xml\n",
      "  10. tfl_21-R68-_-y05-58885_glMYlIz.xml\n",
      "  11. tfl_21-S1-_-y05-59996.xml\n",
      "\n",
      "First 5 files: ['404-21-404-_-y05-2_1.xml', 'tfl_21-404-_-y05-6_qyPH39c.xml', 'tfl_21-404-_-y05-7.xml', 'tfl_21-407-_-y05-11_HygcEjI.xml', 'tfl_21-464-_-y05-55535.xml']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/shitalyadav/Desktop/untitled folder/timetable/Abellio London Ltd_27/404-21-404-_-y05-2_1.xml',\n",
       " '/Users/shitalyadav/Desktop/untitled folder/timetable/Abellio London Ltd_27/tfl_21-404-_-y05-6_qyPH39c.xml',\n",
       " '/Users/shitalyadav/Desktop/untitled folder/timetable/Abellio London Ltd_27/tfl_21-404-_-y05-7.xml',\n",
       " '/Users/shitalyadav/Desktop/untitled folder/timetable/Abellio London Ltd_27/tfl_21-407-_-y05-11_HygcEjI.xml',\n",
       " '/Users/shitalyadav/Desktop/untitled folder/timetable/Abellio London Ltd_27/tfl_21-464-_-y05-55535.xml']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration\n",
    "ROOT_FOLDER = os.path.join(os.getcwd(), \"timetable\", \"Abellio London Ltd_27\")\n",
    "OUTPUT_BASE = os.path.join(os.getcwd(), \"timetable_parsed_data\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "# Scan for XML files\n",
    "xml_files = sorted(glob.glob(os.path.join(ROOT_FOLDER, \"*.xml\")))\n",
    "print(f\"‚úì Found {len(xml_files)} XML files in Abellio London Ltd_27 folder\")\n",
    "print(f\"‚úì Output directory: {OUTPUT_BASE}\\n\")\n",
    "\n",
    "if xml_files:\n",
    "    print(\"Files to process:\")\n",
    "    for i, xml_file in enumerate(xml_files, 1):\n",
    "        print(f\"  {i}. {os.path.basename(xml_file)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No XML files found!\")\n",
    "    \n",
    "print(f\"\\nFirst 5 files: {[os.path.basename(f) for f in xml_files[:5]]}\")\n",
    "xml_files[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd7e5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transxchange_file(xml_file):\n",
    "    \"\"\"\n",
    "    Parse a TransXChange XML file and extract all tables into separate DataFrames\n",
    "    Returns a dictionary with table names as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Define namespace\n",
    "        ns = {'tx': 'http://www.transxchange.org.uk/'}\n",
    "        \n",
    "        # Dictionary to hold all tables\n",
    "        tables = {}\n",
    "        \n",
    "        # 1. Extract Stops (AnnotatedStopPointRef)\n",
    "        stops = []\n",
    "        for stop in root.findall('.//tx:AnnotatedStopPointRef', ns):\n",
    "            stop_ref = stop.find('tx:StopPointRef', ns)\n",
    "            common_name = stop.find('tx:CommonName', ns)\n",
    "            stops.append({\n",
    "                'stop_point_ref': stop_ref.text if stop_ref is not None else None,\n",
    "                'common_name': common_name.text if common_name is not None else None\n",
    "            })\n",
    "        tables['stops'] = pd.DataFrame(stops)\n",
    "        \n",
    "        # 2. Extract Operators\n",
    "        operators = []\n",
    "        for operator in root.findall('.//tx:Operator', ns):\n",
    "            operators.append({\n",
    "                'operator_id': operator.get('id'),\n",
    "                'national_operator_code': operator.findtext('tx:NationalOperatorCode', default=None, namespaces=ns),\n",
    "                'operator_code': operator.findtext('tx:OperatorCode', default=None, namespaces=ns),\n",
    "                'operator_short_name': operator.findtext('tx:OperatorShortName', default=None, namespaces=ns),\n",
    "                'operator_name_on_licence': operator.findtext('tx:OperatorNameOnLicence', default=None, namespaces=ns),\n",
    "                'trading_name': operator.findtext('tx:TradingName', default=None, namespaces=ns),\n",
    "                'licence_number': operator.findtext('tx:LicenceNumber', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['operators'] = pd.DataFrame(operators)\n",
    "        \n",
    "        # 3. Extract Services\n",
    "        services = []\n",
    "        for service in root.findall('.//tx:Service', ns):\n",
    "            services.append({\n",
    "                'service_code': service.findtext('tx:ServiceCode', default=None, namespaces=ns),\n",
    "                'private_code': service.findtext('tx:PrivateCode', default=None, namespaces=ns),\n",
    "                'operator_ref': service.findtext('.//tx:RegisteredOperatorRef', default=None, namespaces=ns),\n",
    "                'start_date': service.findtext('.//tx:StartDate', default=None, namespaces=ns),\n",
    "                'end_date': service.findtext('.//tx:EndDate', default=None, namespaces=ns),\n",
    "                'origin': service.findtext('.//tx:Origin', default=None, namespaces=ns),\n",
    "                'destination': service.findtext('.//tx:Destination', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['services'] = pd.DataFrame(services)\n",
    "        \n",
    "        # 4. Extract Lines\n",
    "        lines = []\n",
    "        for line in root.findall('.//tx:Line', ns):\n",
    "            lines.append({\n",
    "                'line_id': line.get('id'),\n",
    "                'line_name': line.findtext('tx:LineName', default=None, namespaces=ns),\n",
    "                'outbound_origin': line.findtext('.//tx:OutboundDescription/tx:Origin', default=None, namespaces=ns),\n",
    "                'outbound_destination': line.findtext('.//tx:OutboundDescription/tx:Destination', default=None, namespaces=ns),\n",
    "                'inbound_origin': line.findtext('.//tx:InboundDescription/tx:Origin', default=None, namespaces=ns),\n",
    "                'inbound_destination': line.findtext('.//tx:InboundDescription/tx:Destination', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['lines'] = pd.DataFrame(lines)\n",
    "        \n",
    "        # 5. Extract Routes\n",
    "        routes = []\n",
    "        for route in root.findall('.//tx:Route', ns):\n",
    "            routes.append({\n",
    "                'route_id': route.get('id'),\n",
    "                'route_description': route.findtext('tx:Description', default=None, namespaces=ns),\n",
    "                'route_section_ref': route.findtext('.//tx:RouteSectionRef', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['routes'] = pd.DataFrame(routes)\n",
    "        \n",
    "        # 6. Extract Route Links\n",
    "        route_links = []\n",
    "        for route_link in root.findall('.//tx:RouteLink', ns):\n",
    "            route_links.append({\n",
    "                'route_link_id': route_link.get('id'),\n",
    "                'from_stop': route_link.findtext('.//tx:From/tx:StopPointRef', default=None, namespaces=ns),\n",
    "                'to_stop': route_link.findtext('.//tx:To/tx:StopPointRef', default=None, namespaces=ns),\n",
    "                'direction': route_link.findtext('tx:Direction', default=None, namespaces=ns),\n",
    "                'distance': route_link.findtext('tx:Distance', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['route_links'] = pd.DataFrame(route_links)\n",
    "        \n",
    "        # 7. Extract Journey Patterns\n",
    "        journey_patterns = []\n",
    "        for jp in root.findall('.//tx:JourneyPattern', ns):\n",
    "            journey_patterns.append({\n",
    "                'journey_pattern_id': jp.get('id'),\n",
    "                'destination_display': jp.findtext('tx:DestinationDisplay', default=None, namespaces=ns),\n",
    "                'direction': jp.findtext('tx:Direction', default=None, namespaces=ns),\n",
    "                'route_ref': jp.findtext('tx:RouteRef', default=None, namespaces=ns),\n",
    "                'journey_pattern_section_refs': jp.findtext('tx:JourneyPatternSectionRefs', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['journey_patterns'] = pd.DataFrame(journey_patterns)\n",
    "        \n",
    "        # 8. Extract Timing Links (JourneyPatternTimingLink)\n",
    "        timing_links = []\n",
    "        for tl in root.findall('.//tx:JourneyPatternTimingLink', ns):\n",
    "            from_elem = tl.find('tx:From', ns)\n",
    "            to_elem = tl.find('tx:To', ns)\n",
    "            timing_links.append({\n",
    "                'timing_link_id': tl.get('id'),\n",
    "                'from_stop_ref': from_elem.findtext('tx:StopPointRef', default=None, namespaces=ns) if from_elem is not None else None,\n",
    "                'from_sequence': from_elem.get('SequenceNumber') if from_elem is not None else None,\n",
    "                'from_timing_status': from_elem.findtext('tx:TimingStatus', default=None, namespaces=ns) if from_elem is not None else None,\n",
    "                'to_stop_ref': to_elem.findtext('tx:StopPointRef', default=None, namespaces=ns) if to_elem is not None else None,\n",
    "                'to_sequence': to_elem.get('SequenceNumber') if to_elem is not None else None,\n",
    "                'to_timing_status': to_elem.findtext('tx:TimingStatus', default=None, namespaces=ns) if to_elem is not None else None,\n",
    "                'route_link_ref': tl.findtext('tx:RouteLinkRef', default=None, namespaces=ns),\n",
    "                'run_time': tl.findtext('tx:RunTime', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['timing_links'] = pd.DataFrame(timing_links)\n",
    "        \n",
    "        # 9. Extract Vehicle Journeys\n",
    "        vehicle_journeys = []\n",
    "        for vj in root.findall('.//tx:VehicleJourney', ns):\n",
    "            # Extract operating days\n",
    "            days_of_week = []\n",
    "            for day_elem in vj.findall('.//tx:DaysOfWeek/*', ns):\n",
    "                days_of_week.append(day_elem.tag.split('}')[-1])\n",
    "            \n",
    "            vehicle_journeys.append({\n",
    "                'vehicle_journey_code': vj.findtext('tx:VehicleJourneyCode', default=None, namespaces=ns),\n",
    "                'private_code': vj.findtext('tx:PrivateCode', default=None, namespaces=ns),\n",
    "                'service_ref': vj.findtext('tx:ServiceRef', default=None, namespaces=ns),\n",
    "                'line_ref': vj.findtext('tx:LineRef', default=None, namespaces=ns),\n",
    "                'journey_pattern_ref': vj.findtext('tx:JourneyPatternRef', default=None, namespaces=ns),\n",
    "                'departure_time': vj.findtext('tx:DepartureTime', default=None, namespaces=ns),\n",
    "                'journey_code': vj.findtext('.//tx:JourneyCode', default=None, namespaces=ns),\n",
    "                'days_of_week': ','.join(days_of_week) if days_of_week else None,\n",
    "                'sequence_number': vj.get('SequenceNumber')\n",
    "            })\n",
    "        tables['vehicle_journeys'] = pd.DataFrame(vehicle_journeys)\n",
    "        \n",
    "        # 10. Extract Serviced Organisations\n",
    "        serviced_orgs = []\n",
    "        for org in root.findall('.//tx:ServicedOrganisation', ns):\n",
    "            serviced_orgs.append({\n",
    "                'organisation_code': org.findtext('tx:OrganisationCode', default=None, namespaces=ns),\n",
    "                'name': org.findtext('tx:Name', default=None, namespaces=ns)\n",
    "            })\n",
    "        tables['serviced_organisations'] = pd.DataFrame(serviced_orgs)\n",
    "        \n",
    "        # 11. Extract Serviced Organisation Working Days\n",
    "        working_days = []\n",
    "        for org in root.findall('.//tx:ServicedOrganisation', ns):\n",
    "            org_code = org.findtext('tx:OrganisationCode', default=None, namespaces=ns)\n",
    "            for date_range in org.findall('.//tx:DateRange', ns):\n",
    "                working_days.append({\n",
    "                    'organisation_code': org_code,\n",
    "                    'start_date': date_range.findtext('tx:StartDate', default=None, namespaces=ns),\n",
    "                    'end_date': date_range.findtext('tx:EndDate', default=None, namespaces=ns),\n",
    "                    'description': date_range.findtext('tx:Description', default=None, namespaces=ns)\n",
    "                })\n",
    "        tables['serviced_org_working_days'] = pd.DataFrame(working_days)\n",
    "        \n",
    "        # 12. Extract Operating Profiles\n",
    "        operating_profiles = []\n",
    "        for vj in root.findall('.//tx:VehicleJourney', ns):\n",
    "            vj_code = vj.findtext('tx:VehicleJourneyCode', default=None, namespaces=ns)\n",
    "            op_profile = vj.find('tx:OperatingProfile', ns)\n",
    "            \n",
    "            if op_profile is not None:\n",
    "                # Regular days\n",
    "                days_of_week = []\n",
    "                for day_elem in op_profile.findall('.//tx:DaysOfWeek/*', ns):\n",
    "                    days_of_week.append(day_elem.tag.split('}')[-1])\n",
    "                \n",
    "                # Bank holidays\n",
    "                bank_holidays_operation = []\n",
    "                for bh in op_profile.findall('.//tx:BankHolidayOperation/tx:DaysOfOperation/*', ns):\n",
    "                    bank_holidays_operation.append(bh.tag.split('}')[-1])\n",
    "                \n",
    "                bank_holidays_non_operation = []\n",
    "                for bh in op_profile.findall('.//tx:BankHolidayOperation/tx:DaysOfNonOperation/*', ns):\n",
    "                    bank_holidays_non_operation.append(bh.tag.split('}')[-1])\n",
    "                \n",
    "                operating_profiles.append({\n",
    "                    'vehicle_journey_code': vj_code,\n",
    "                    'days_of_week': ','.join(days_of_week) if days_of_week else None,\n",
    "                    'bank_holidays_operation': ','.join(bank_holidays_operation) if bank_holidays_operation else None,\n",
    "                    'bank_holidays_non_operation': ','.join(bank_holidays_non_operation) if bank_holidays_non_operation else None\n",
    "                })\n",
    "        tables['operating_profiles'] = pd.DataFrame(operating_profiles)\n",
    "        \n",
    "        # 13. Extract Service Journey Patterns (mapping services to journey patterns)\n",
    "        service_journey_patterns = []\n",
    "        for service in root.findall('.//tx:Service', ns):\n",
    "            service_code = service.findtext('tx:ServiceCode', default=None, namespaces=ns)\n",
    "            for jp in service.findall('.//tx:JourneyPattern', ns):\n",
    "                service_journey_patterns.append({\n",
    "                    'service_code': service_code,\n",
    "                    'journey_pattern_id': jp.get('id'),\n",
    "                    'destination_display': jp.findtext('tx:DestinationDisplay', default=None, namespaces=ns)\n",
    "                })\n",
    "        tables['service_journey_patterns'] = pd.DataFrame(service_journey_patterns)\n",
    "        \n",
    "        # 14. Extract Service Lines (mapping services to lines)\n",
    "        service_lines = []\n",
    "        for service in root.findall('.//tx:Service', ns):\n",
    "            service_code = service.findtext('tx:ServiceCode', default=None, namespaces=ns)\n",
    "            for line in service.findall('.//tx:Line', ns):\n",
    "                service_lines.append({\n",
    "                    'service_code': service_code,\n",
    "                    'line_id': line.get('id'),\n",
    "                    'line_name': line.findtext('tx:LineName', default=None, namespaces=ns)\n",
    "                })\n",
    "        tables['service_lines'] = pd.DataFrame(service_lines)\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {xml_file}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb9b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: 404-21-404-_-y05-2_1.xml\n",
      "================================================================================\n",
      "\n",
      "‚úì Successfully parsed 404-21-404-_-y05-2_1.xml\n",
      "\n",
      "Extracted 14 tables:\n",
      "\n",
      "  üìä stops                          - 77 rows\n",
      "  üìä operators                      - 1 rows\n",
      "  üìä services                       - 1 rows\n",
      "  üìä lines                          - 1 rows\n",
      "  üìä routes                         - 2 rows\n",
      "  üìä route_links                    - 88 rows\n",
      "  üìä journey_patterns               - 19 rows\n",
      "  üìä timing_links                   - 836 rows\n",
      "  üìä vehicle_journeys               - 204 rows\n",
      "  üìä serviced_organisations         - 1 rows\n",
      "  üìä serviced_org_working_days      - 6 rows\n",
      "  üìä operating_profiles             - 204 rows\n",
      "  üìä service_journey_patterns       - 19 rows\n",
      "  üìä service_lines                  - 1 rows\n",
      "\n",
      "================================================================================\n",
      "Sample Data Preview:\n",
      "================================================================================\n",
      "\n",
      "üöè STOPS (first 5):\n",
      "  stop_point_ref                       common_name\n",
      "0     490002178Z  Crawford Crescent (->S) Coulsdon\n",
      "1     490002177Z   Shaftesbury Lane (->E) Coulsdon\n",
      "2     490002176Z       Salter Close (->E) Coulsdon\n",
      "3     490002175Z        Tullick Way (->E) Coulsdon\n",
      "4     490002174Z   Lime Tree Avenue (->S) Coulsdon\n",
      "\n",
      "üöå OPERATORS:\n",
      "  operator_id national_operator_code operator_code     operator_short_name  \\\n",
      "0          CX                   ABLO            CX  ABELLIO LONDON LIMITED   \n",
      "\n",
      "  operator_name_on_licence            trading_name licence_number  \n",
      "0   ABELLIO LONDON LIMITED  ABELLIO LONDON LIMITED      PK0003436  \n",
      "\n",
      "üõ§Ô∏è  LINES:\n",
      "                line_id line_name                   outbound_origin  \\\n",
      "0  ABLO:PK0003436:5:404       404  Crawford Crescent (->S) Coulsdon   \n",
      "\n",
      "            outbound_destination                 inbound_origin  \\\n",
      "0  Westway Common (opp) Caterham  Westway Common (opp) Caterham   \n",
      "\n",
      "                inbound_destination  \n",
      "0  Crawford Crescent (->S) Coulsdon  \n",
      "\n",
      "üöê VEHICLE JOURNEYS (first 5):\n",
      "  vehicle_journey_code                           private_code  service_ref  \\\n",
      "0                  VJ1  21-404-_-y05-2SU:O:0:128:El6_hUK9Ri0=  PK0003436:5   \n",
      "1                  VJ2  21-404-_-y05-2SU:O:0:129:El6_hUK9Ri0=  PK0003436:5   \n",
      "2                  VJ3  21-404-_-y05-2SU:I:0:103:El6_hUK9Ri0=  PK0003436:5   \n",
      "3                  VJ4  21-404-_-y05-2SU:O:0:130:El6_hUK9Ri0=  PK0003436:5   \n",
      "4                  VJ5  21-404-_-y05-2SU:I:0:104:El6_hUK9Ri0=  PK0003436:5   \n",
      "\n",
      "               line_ref journey_pattern_ref departure_time journey_code  \\\n",
      "0  ABLO:PK0003436:5:404                JP15       06:25:00            2   \n",
      "1  ABLO:PK0003436:5:404                JP15       06:55:00            4   \n",
      "2  ABLO:PK0003436:5:404                JP11       07:10:00            1   \n",
      "3  ABLO:PK0003436:5:404                JP15       07:25:00            6   \n",
      "4  ABLO:PK0003436:5:404                JP11       07:40:00            3   \n",
      "\n",
      "  days_of_week sequence_number  \n",
      "0       Sunday               1  \n",
      "1       Sunday               2  \n",
      "2       Sunday               3  \n",
      "3       Sunday               4  \n",
      "4       Sunday               5  \n"
     ]
    }
   ],
   "source": [
    "# Test with the first XML file\n",
    "if xml_files:\n",
    "    test_file = xml_files[0]\n",
    "    print(f\"Testing with: {os.path.basename(test_file)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    tables = parse_transxchange_file(test_file)\n",
    "    \n",
    "    if tables:\n",
    "        print(f\"\\n‚úì Successfully parsed {os.path.basename(test_file)}\")\n",
    "        print(f\"\\nExtracted {len(tables)} tables:\\n\")\n",
    "        \n",
    "        for table_name, df in tables.items():\n",
    "            print(f\"  üìä {table_name:30s} - {len(df):,} rows\")\n",
    "        \n",
    "        # Show sample data from key tables\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"Sample Data Preview:\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Preview Stops\n",
    "        if not tables['stops'].empty:\n",
    "            print(\"üöè STOPS (first 5):\")\n",
    "            print(tables['stops'].head())\n",
    "            print()\n",
    "        \n",
    "        # Preview Operators\n",
    "        if not tables['operators'].empty:\n",
    "            print(\"üöå OPERATORS:\")\n",
    "            print(tables['operators'])\n",
    "            print()\n",
    "        \n",
    "        # Preview Lines\n",
    "        if not tables['lines'].empty:\n",
    "            print(\"üõ§Ô∏è  LINES:\")\n",
    "            print(tables['lines'])\n",
    "            print()\n",
    "        \n",
    "        # Preview Vehicle Journeys\n",
    "        if not tables['vehicle_journeys'].empty:\n",
    "            print(\"üöê VEHICLE JOURNEYS (first 5):\")\n",
    "            print(tables['vehicle_journeys'].head())\n",
    "    else:\n",
    "        print(\"‚ùå Failed to parse file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No XML files found to test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1883f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing ALL Abellio London Ltd XML files...\n",
      "Total files to process: 11\n",
      "\n",
      "[1/11] Processing: 404-21-404-_-y05-2_1.xml... ‚úì Done (1,460 records)\n",
      "[2/11] Processing: tfl_21-404-_-y05-6_qyPH39c.xml... ‚úì Done (1,354 records)\n",
      "‚úì Done (1,352 records)_21-404-_-y05-7.xml... \n",
      "‚úì Done (22,136 records)21-407-_-y05-11_HygcEjI.xml... \n",
      "[5/11] Processing: tfl_21-464-_-y05-55535.xml... ‚úì Done (921 records)\n",
      "[6/11] Processing: tfl_21-464-_-y05-55538.xml... ‚úì Done (1,099 records)\n",
      "‚úì Done (1,221 records)_21-464-_-y05-55539.xml... \n",
      "‚úì Done (8,225 records)_21-465-_-y05-7_b3z8CFQ.xml... \n",
      "‚úì Done (11,150 records)21-R68-_-y05-58885.xml... \n",
      "‚úì Done (11,150 records)_21-R68-_-y05-58885_glMYlIz.xml... \n",
      "‚úì Done (8,806 records)l_21-S1-_-y05-59996.xml... \n",
      "\n",
      "================================================================================\n",
      "Processing complete!\n",
      "  ‚úì Successful: 11/11\n",
      "  ‚ùå Failed: 0/11\n",
      "================================================================================\n",
      "\n",
      "üìä Consolidated Data Summary:\n",
      "\n",
      "  stops                          - 931 rows\n",
      "  operators                      - 11 rows\n",
      "  services                       - 11 rows\n",
      "  lines                          - 11 rows\n",
      "  routes                         - 27 rows\n",
      "  route_links                    - 958 rows\n",
      "  journey_patterns               - 1,189 rows\n",
      "  timing_links                   - 58,840 rows\n",
      "  vehicle_journeys               - 2,836 rows\n",
      "  serviced_organisations         - 5 rows\n",
      "  serviced_org_working_days      - 19 rows\n",
      "  operating_profiles             - 2,836 rows\n",
      "  service_journey_patterns       - 1,189 rows\n",
      "  service_lines                  - 11 rows\n"
     ]
    }
   ],
   "source": [
    "# Process ALL XML files and consolidate data\n",
    "print(\"üöÄ Processing ALL Abellio London Ltd XML files...\")\n",
    "print(f\"Total files to process: {len(xml_files)}\\n\")\n",
    "\n",
    "# Dictionary to accumulate all tables across all files\n",
    "all_tables = {\n",
    "    'stops': [],\n",
    "    'operators': [],\n",
    "    'services': [],\n",
    "    'lines': [],\n",
    "    'routes': [],\n",
    "    'route_links': [],\n",
    "    'journey_patterns': [],\n",
    "    'timing_links': [],\n",
    "    'vehicle_journeys': [],\n",
    "    'serviced_organisations': [],\n",
    "    'serviced_org_working_days': [],\n",
    "    'operating_profiles': [],\n",
    "    'service_journey_patterns': [],\n",
    "    'service_lines': []\n",
    "}\n",
    "\n",
    "# Process each XML file\n",
    "successful_files = 0\n",
    "failed_files = 0\n",
    "\n",
    "for i, xml_file in enumerate(xml_files, 1):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    print(f\"[{i}/{len(xml_files)}] Processing: {filename}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        tables = parse_transxchange_file(xml_file)\n",
    "        \n",
    "        if tables:\n",
    "            # Add source file column and accumulate data\n",
    "            for table_name, df in tables.items():\n",
    "                if not df.empty:\n",
    "                    df['source_file'] = filename\n",
    "                    all_tables[table_name].append(df)\n",
    "            \n",
    "            print(f\"‚úì Done ({sum(len(df) for df in tables.values()):,} records)\")\n",
    "            successful_files += 1\n",
    "        else:\n",
    "            print(\"‚ùå Failed\")\n",
    "            failed_files += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        failed_files += 1\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Processing complete!\")\n",
    "print(f\"  ‚úì Successful: {successful_files}/{len(xml_files)}\")\n",
    "print(f\"  ‚ùå Failed: {failed_files}/{len(xml_files)}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Consolidate all dataframes\n",
    "consolidated_tables = {}\n",
    "for table_name, df_list in all_tables.items():\n",
    "    if df_list:\n",
    "        consolidated_tables[table_name] = pd.concat(df_list, ignore_index=True)\n",
    "    else:\n",
    "        consolidated_tables[table_name] = pd.DataFrame()\n",
    "\n",
    "# Display summary\n",
    "print(\"üìä Consolidated Data Summary:\\n\")\n",
    "for table_name, df in consolidated_tables.items():\n",
    "    if not df.empty:\n",
    "        print(f\"  {table_name:30s} - {len(df):,} rows\")\n",
    "    else:\n",
    "        print(f\"  {table_name:30s} - Empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b3e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving tables to CSV files...\n",
      "\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/stops/stops_abellio_london.csv\n",
      "    ‚îî‚îÄ 931 rows, 3 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/operators/operators_abellio_london.csv\n",
      "    ‚îî‚îÄ 11 rows, 8 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/services/services_abellio_london.csv\n",
      "    ‚îî‚îÄ 11 rows, 8 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/lines/lines_abellio_london.csv\n",
      "    ‚îî‚îÄ 11 rows, 7 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/routes/routes_abellio_london.csv\n",
      "    ‚îî‚îÄ 27 rows, 4 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/route_links/route_links_abellio_london.csv\n",
      "    ‚îî‚îÄ 958 rows, 6 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/journey_patterns/journey_patterns_abellio_london.csv\n",
      "    ‚îî‚îÄ 1,189 rows, 6 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/timing_links/timing_links_abellio_london.csv\n",
      "    ‚îî‚îÄ 58,840 rows, 10 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/vehicle_journeys/vehicle_journeys_abellio_london.csv\n",
      "    ‚îî‚îÄ 2,836 rows, 10 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/serviced_organisations/serviced_organisations_abellio_london.csv\n",
      "    ‚îî‚îÄ 5 rows, 3 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/serviced_org_working_days/serviced_org_working_days_abellio_london.csv\n",
      "    ‚îî‚îÄ 19 rows, 5 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/operating_profiles/operating_profiles_abellio_london.csv\n",
      "    ‚îî‚îÄ 2,836 rows, 5 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/service_journey_patterns/service_journey_patterns_abellio_london.csv\n",
      "    ‚îî‚îÄ 1,189 rows, 4 columns\n",
      "  ‚úì Saved: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/service_lines/service_lines_abellio_london.csv\n",
      "    ‚îî‚îÄ 11 rows, 4 columns\n",
      "\n",
      "================================================================================\n",
      "Export complete!\n",
      "  ‚úì Saved: 14 tables\n",
      "  ‚äò Empty: 0 tables\n",
      "  üìÅ Output location: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save all tables to CSV files\n",
    "print(\"üíæ Saving tables to CSV files...\\n\")\n",
    "\n",
    "saved_count = 0\n",
    "empty_count = 0\n",
    "\n",
    "for table_name, df in consolidated_tables.items():\n",
    "    # Create subdirectory for each table type\n",
    "    output_dir = os.path.join(OUTPUT_BASE, table_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Save to CSV with timestamp\n",
    "        csv_filename = f\"{table_name}_abellio_london.csv\"\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"  ‚úì Saved: {csv_path}\")\n",
    "        print(f\"    ‚îî‚îÄ {len(df):,} rows, {len(df.columns)} columns\")\n",
    "        saved_count += 1\n",
    "    else:\n",
    "        print(f\"  ‚äò Skipped: {table_name} (empty)\")\n",
    "        empty_count += 1\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Export complete!\")\n",
    "print(f\"  ‚úì Saved: {saved_count} tables\")\n",
    "print(f\"  ‚äò Empty: {empty_count} tables\")\n",
    "print(f\"  üìÅ Output location: {OUTPUT_BASE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2af6639-f0de-4937-81dc-0864051e0ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying saved CSV files...\n",
      "\n",
      "‚úì stops\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/stops/stops_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 931 | Columns: 3 | Size: 55.5 KB\n",
      "  ‚îî‚îÄ Columns: stop_point_ref, common_name, source_file\n",
      "\n",
      "‚úì operators\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/operators/operators_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 11 | Columns: 8 | Size: 1.1 KB\n",
      "  ‚îî‚îÄ Columns: operator_id, national_operator_code, operator_code, operator_short_name, operator_name_on_licence...\n",
      "\n",
      "‚úì services\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/services/services_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 11 | Columns: 8 | Size: 1.4 KB\n",
      "  ‚îî‚îÄ Columns: service_code, private_code, operator_ref, start_date, end_date...\n",
      "\n",
      "‚úì lines\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/lines/lines_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 11 | Columns: 7 | Size: 0.8 KB\n",
      "\n",
      "‚úì routes\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/routes/routes_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 27 | Columns: 4 | Size: 3.2 KB\n",
      "\n",
      "‚úì route_links\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/route_links/route_links_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 958 | Columns: 6 | Size: 76.1 KB\n",
      "\n",
      "‚úì journey_patterns\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/journey_patterns/journey_patterns_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 1,189 | Columns: 6 | Size: 151.1 KB\n",
      "\n",
      "‚úì timing_links\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/timing_links/timing_links_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 58,840 | Columns: 10 | Size: 7555.5 KB\n",
      "\n",
      "‚úì vehicle_journeys\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/vehicle_journeys/vehicle_journeys_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 2,836 | Columns: 10 | Size: 501.2 KB\n",
      "  ‚îî‚îÄ Columns: vehicle_journey_code, private_code, service_ref, line_ref, journey_pattern_ref...\n",
      "\n",
      "‚úì serviced_organisations\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/serviced_organisations/serviced_organisations_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 5 | Columns: 3 | Size: 0.5 KB\n",
      "\n",
      "‚úì serviced_org_working_days\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/serviced_org_working_days/serviced_org_working_days_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 19 | Columns: 5 | Size: 1.6 KB\n",
      "\n",
      "‚úì operating_profiles\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/operating_profiles/operating_profiles_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 2,836 | Columns: 5 | Size: 565.8 KB\n",
      "\n",
      "‚úì service_journey_patterns\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/service_journey_patterns/service_journey_patterns_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 1,189 | Columns: 4 | Size: 96.6 KB\n",
      "\n",
      "‚úì service_lines\n",
      "  ‚îî‚îÄ Path: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data/service_lines/service_lines_abellio_london.csv\n",
      "  ‚îî‚îÄ Rows: 11 | Columns: 4 | Size: 0.8 KB\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All CSV files verified successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved CSV files\n",
    "print(\"üîç Verifying saved CSV files...\\n\")\n",
    "\n",
    "for table_name in consolidated_tables.keys():\n",
    "    output_dir = os.path.join(OUTPUT_BASE, table_name)\n",
    "    csv_path = os.path.join(output_dir, f\"{table_name}_abellio_london.csv\")\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        # Read back and verify\n",
    "        df_verify = pd.read_csv(csv_path)\n",
    "        file_size = os.path.getsize(csv_path) / 1024  # Size in KB\n",
    "        \n",
    "        print(f\"‚úì {table_name}\")\n",
    "        print(f\"  ‚îî‚îÄ Path: {csv_path}\")\n",
    "        print(f\"  ‚îî‚îÄ Rows: {len(df_verify):,} | Columns: {len(df_verify.columns)} | Size: {file_size:.1f} KB\")\n",
    "        \n",
    "        # Show column names for important tables\n",
    "        if table_name in ['stops', 'operators', 'vehicle_journeys', 'services']:\n",
    "            print(f\"  ‚îî‚îÄ Columns: {', '.join(df_verify.columns[:5])}{'...' if len(df_verify.columns) > 5 else ''}\")\n",
    "        print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ All CSV files verified successfully!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122d0b98-987b-4cd5-9787-4e1b1716fbc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Sample Data from Key Tables\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üöå OPERATORS:\n",
      "   operator_id     operator_short_name licence_number\n",
      "0           CX  ABELLIO LONDON LIMITED      PK0003436\n",
      "1       OId_CX            Transport UK      UZ000ABLO\n",
      "3       OId_CX            Transport UK      PK0003436\n",
      "7       OId_TE            Transport UK      PK0001815\n",
      "10      OId_CX            Transport UK      PK0001816\n",
      "\n",
      "üõ§Ô∏è  LINES SUMMARY:\n",
      "Total unique lines: 7\n",
      "   line_name                 line_id\n",
      "0        404    ABLO:PK0003436:5:404\n",
      "1        404  ABLO:UZ000ABLO:404:404\n",
      "3        407    ABLO:PK0003436:2:407\n",
      "4        464    ABLO:PK0003436:4:464\n",
      "7        465   ABLO:PK0001815:90:465\n",
      "8        R68   ABLO:PK0001815:15:R68\n",
      "10        S1   ABLO:PK0001816:107:S1\n",
      "\n",
      "üöè STOPS STATISTICS:\n",
      "Total unique stops: 570\n",
      "\n",
      "Sample stops:\n",
      "  stop_point_ref                             common_name\n",
      "0     490002178Z        Crawford Crescent (->S) Coulsdon\n",
      "1     490002177Z         Shaftesbury Lane (->E) Coulsdon\n",
      "2     490002176Z             Salter Close (->E) Coulsdon\n",
      "3     490002175Z              Tullick Way (->E) Coulsdon\n",
      "4     490002174Z         Lime Tree Avenue (->S) Coulsdon\n",
      "5     490002189Z             Farthing Way (->N) Coulsdon\n",
      "6     490001253B         Coulsdon Town (Stop B) Coulsdon\n",
      "7     490005637C      Coulsdon Library (Stop C) Coulsdon\n",
      "8     490009202D       Lion Green Road (Stop D) Coulsdon\n",
      "9     490001072M  Coulsdon South (Stop M) Coulsdon South\n",
      "\n",
      "üöê VEHICLE JOURNEYS STATISTICS:\n",
      "Total vehicle journeys: 2,836\n",
      "Unique departure times: 594\n",
      "\n",
      "Journeys by day of week:\n",
      "  Monday,Tuesday,Wednesday,Thursday,Friday: 1,125\n",
      "  Saturday: 944\n",
      "  Sunday: 767\n",
      "\n",
      "üìÖ SERVICES SUMMARY:\n",
      "Total services: 11\n",
      "Unique service codes: 7\n",
      "\n",
      "Sample services:\n",
      "    service_code                            origin  \\\n",
      "0    PK0003436:5  Crawford Crescent (->S) Coulsdon   \n",
      "1  UZ000ABLO:404                 Crawford Crescent   \n",
      "2  UZ000ABLO:404                 Crawford Crescent   \n",
      "3    PK0003436:2           Sutton / Marshalls Road   \n",
      "4    PK0003436:4      Tatsfield Village / Old Ship   \n",
      "\n",
      "                     destination  \n",
      "0  Westway Common (opp) Caterham  \n",
      "1               Westway Caterham  \n",
      "2               Westway Caterham  \n",
      "3                Caterham Valley  \n",
      "4        New Addington Tram Stop  \n",
      "\n",
      "================================================================================\n",
      "‚úÖ Data extraction complete!\n",
      "================================================================================\n",
      "\n",
      "üìÅ All CSV files saved in: /Users/shitalyadav/Desktop/untitled folder/timetable_parsed_data\n",
      "\n",
      "üìä Summary of extracted tables:\n",
      "  ‚Ä¢ stops: 931 rows\n",
      "  ‚Ä¢ operators: 11 rows\n",
      "  ‚Ä¢ services: 11 rows\n",
      "  ‚Ä¢ lines: 11 rows\n",
      "  ‚Ä¢ routes: 27 rows\n",
      "  ‚Ä¢ route_links: 958 rows\n",
      "  ‚Ä¢ journey_patterns: 1,189 rows\n",
      "  ‚Ä¢ timing_links: 58,840 rows\n",
      "  ‚Ä¢ vehicle_journeys: 2,836 rows\n",
      "  ‚Ä¢ serviced_organisations: 5 rows\n",
      "  ‚Ä¢ serviced_org_working_days: 19 rows\n",
      "  ‚Ä¢ operating_profiles: 2,836 rows\n",
      "  ‚Ä¢ service_journey_patterns: 1,189 rows\n",
      "  ‚Ä¢ service_lines: 11 rows\n"
     ]
    }
   ],
   "source": [
    "# Display sample data from key tables\n",
    "print(\"üìã Sample Data from Key Tables\\n\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# 1. Operators\n",
    "if not consolidated_tables['operators'].empty:\n",
    "    print(\"üöå OPERATORS:\")\n",
    "    print(consolidated_tables['operators'][['operator_id', 'operator_short_name', 'licence_number']].drop_duplicates())\n",
    "    print()\n",
    "\n",
    "# 2. Lines Summary\n",
    "if not consolidated_tables['lines'].empty:\n",
    "    print(\"üõ§Ô∏è  LINES SUMMARY:\")\n",
    "    lines_summary = consolidated_tables['lines'][['line_name', 'line_id']].drop_duplicates()\n",
    "    print(f\"Total unique lines: {len(lines_summary)}\")\n",
    "    print(lines_summary.head(10))\n",
    "    print()\n",
    "\n",
    "# 3. Stops Statistics\n",
    "if not consolidated_tables['stops'].empty:\n",
    "    print(\"üöè STOPS STATISTICS:\")\n",
    "    print(f\"Total unique stops: {consolidated_tables['stops']['stop_point_ref'].nunique():,}\")\n",
    "    print(\"\\nSample stops:\")\n",
    "    print(consolidated_tables['stops'][['stop_point_ref', 'common_name']].drop_duplicates().head(10))\n",
    "    print()\n",
    "\n",
    "# 4. Vehicle Journeys Statistics\n",
    "if not consolidated_tables['vehicle_journeys'].empty:\n",
    "    print(\"üöê VEHICLE JOURNEYS STATISTICS:\")\n",
    "    vj_df = consolidated_tables['vehicle_journeys']\n",
    "    print(f\"Total vehicle journeys: {len(vj_df):,}\")\n",
    "    print(f\"Unique departure times: {vj_df['departure_time'].nunique():,}\")\n",
    "    print(f\"\\nJourneys by day of week:\")\n",
    "    if 'days_of_week' in vj_df.columns:\n",
    "        days_counts = vj_df['days_of_week'].value_counts().head(10)\n",
    "        for day, count in days_counts.items():\n",
    "            print(f\"  {day}: {count:,}\")\n",
    "    print()\n",
    "\n",
    "# 5. Services Summary\n",
    "if not consolidated_tables['services'].empty:\n",
    "    print(\"üìÖ SERVICES SUMMARY:\")\n",
    "    services_df = consolidated_tables['services']\n",
    "    print(f\"Total services: {len(services_df):,}\")\n",
    "    print(f\"Unique service codes: {services_df['service_code'].nunique()}\")\n",
    "    print(\"\\nSample services:\")\n",
    "    print(services_df[['service_code', 'origin', 'destination']].head(5))\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ Data extraction complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüìÅ All CSV files saved in: {OUTPUT_BASE}\")\n",
    "print(f\"\\nüìä Summary of extracted tables:\")\n",
    "for table_name, df in consolidated_tables.items():\n",
    "    if not df.empty:\n",
    "        print(f\"  ‚Ä¢ {table_name}: {len(df):,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c9126-c8b6-4a00-8d8c-6c32a418ff51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db13da2-244a-4809-95f8-860e73541edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357b1e9-1ba0-4836-a5c2-ddee6b0a432f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b4730-52c9-4628-8001-f6a4ea285d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b284f88-fed8-419a-9072-9fbb06e03147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3ad09-7fe8-436e-9c8a-fe9ec7f70342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8c621-2b97-4fc6-91af-e4ae673175e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448a349-ea83-40d0-a7d7-d43ba3fa0655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
